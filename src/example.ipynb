{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:28:23.395321300Z",
     "start_time": "2024-12-03T08:28:23.383819200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mg_dict:[1.0, 4.0]\n",
      "share_ratio_list:[1.0, 0.8]\n",
      "share_ratio_list:[1.0, 0.8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "from multi_gran_generator import creat_coarse_data, creat_coarse_data_elec\n",
    "from mgtsd_estimator import mgtsdEstimator\n",
    "from trainer import Trainer\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import ast\n",
    "from utils import plot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# project_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n",
    "project_path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "\n",
    "alias = {\n",
    "    'elec': 'electricity_nips',\n",
    "    'wiki': 'wiki-rolling_nips',\n",
    "    'cup': 'kdd_cup_2018_without_missing',\n",
    "    'solar': 'solar_nips',\n",
    "    'traf': 'traffic_nips',\n",
    "    'taxi': 'taxi_30min'\n",
    "}\n",
    "input_size_all = {\n",
    "    'solar': 552,\n",
    "    'cup': 1084,\n",
    "    'traf': 3856,\n",
    "    'taxi': 7290,\n",
    "    'elec': 1484,\n",
    "    'wiki': 8002,\n",
    "}\n",
    "feature_size_all = {\n",
    "    'fred': 107,\n",
    "    'solar': 137,\n",
    "    'cup': 270,\n",
    "    'traf': 963,\n",
    "    'taxi': 1214,\n",
    "    'elec': 370,\n",
    "    'wiki': 2000,\n",
    "}\n",
    "\n",
    "model_name = \"mgtsd\"\n",
    "cuda_num = 0\n",
    "dataset_name = \"elec\"\n",
    "epoch = 30\n",
    "diff_steps = 100\n",
    "num_gran = 2\n",
    "mg_dict = \"1_4\"\n",
    "share_ratio_list = \"1_0.8\"\n",
    "weight_list = \"0.8_0.2\"\n",
    "\n",
    "input_size = input_size_all[dataset_name]\n",
    "batch_size = 128\n",
    "mg_dict = [float(i) for i in str(mg_dict).split('_')]\n",
    "print(f\"mg_dict:{mg_dict}\")\n",
    "share_ratio_list = [float(i) for i in str(share_ratio_list).split('_')]\n",
    "print(f\"share_ratio_list:{share_ratio_list}\")\n",
    "weight_list = [float(i) for i in str(weight_list).split('_')]\n",
    "weights = weight_list\n",
    "print(f\"share_ratio_list:{share_ratio_list}\")\n",
    "learning_rate = 1e-05\n",
    "num_cells = 128\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    f\"cuda:{cuda_num}\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:28:30.633621Z",
     "start_time": "2024-12-03T08:28:23.397820400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "prepare the dataset\n",
      "dataset_train0_len:5832\n",
      "dataset_test0_len:4144\n"
     ]
    }
   ],
   "source": [
    "print(\"================================================\")\n",
    "print(\"prepare the dataset\")\n",
    "\n",
    "DATASET_PATH=project_path+os.sep+'datasets'\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.makedirs(DATASET_PATH)\n",
    "dataset = get_dataset(alias[dataset_name],path=Path(DATASET_PATH), regenerate=False)\n",
    "\n",
    "train_grouper = MultivariateGrouper(max_target_dim=min(\n",
    "    2000, int(dataset.metadata.feat_static_cat[0].cardinality)))\n",
    "\n",
    "test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(dataset.train)),\n",
    "                                   max_target_dim=min(2000, int(dataset.metadata.feat_static_cat[0].cardinality)))\n",
    "\n",
    "dataset_train = train_grouper(dataset.train)\n",
    "dataset_test = test_grouper(dataset.test)\n",
    "if dataset_name == 'elec':\n",
    "    data_train, data_test = creat_coarse_data_elec(dataset_train=dataset_train,\n",
    "                                                   dataset_test=dataset_test,\n",
    "                                                   mg_dict=mg_dict)\n",
    "else:\n",
    "    data_train, data_test = creat_coarse_data(dataset_train=dataset_train,\n",
    "                                              dataset_test=dataset_test,\n",
    "                                              mg_dict=mg_dict)\n",
    "print(f'dataset_train0_len:{len(data_train[0][\"target\"][0])}')\n",
    "print(f'dataset_test0_len:{len(data_test[0][\"target\"][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:29:16.792257900Z",
     "start_time": "2024-12-03T08:28:30.642673400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "initlize the estimator\n",
      "self.log_metrics: None\n",
      "================================================\n",
      "start training the network\n",
      "self_share_ratio of estimator:[1.0, 0.8]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/99 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1814e7266d0f45739d1edf9f5eec0a70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 142.00 MiB (GPU 0; 16.00 GiB total capacity; 15.07 GiB already allocated; 0 bytes free; 15.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m================================================\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart training the network\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 29\u001B[0m predictor \u001B[38;5;241m=\u001B[39m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m   \u001B[49m\u001B[43mdata_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43mprefetch_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m===============================================\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake predictions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\estimator.py:185\u001B[0m, in \u001B[0;36mPyTorchEstimator.train\u001B[1;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001B[0m\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    177\u001B[0m     training_data: Dataset,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    184\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m PyTorchPredictor:\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_model(\n\u001B[0;32m    186\u001B[0m         training_data,\n\u001B[0;32m    187\u001B[0m         validation_data,\n\u001B[0;32m    188\u001B[0m         num_workers\u001B[38;5;241m=\u001B[39mnum_workers,\n\u001B[0;32m    189\u001B[0m         prefetch_factor\u001B[38;5;241m=\u001B[39mprefetch_factor,\n\u001B[0;32m    190\u001B[0m         shuffle_buffer_length\u001B[38;5;241m=\u001B[39mshuffle_buffer_length,\n\u001B[0;32m    191\u001B[0m         cache_data\u001B[38;5;241m=\u001B[39mcache_data,\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    193\u001B[0m     )\u001B[38;5;241m.\u001B[39mpredictor\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\estimator.py:153\u001B[0m, in \u001B[0;36mPyTorchEstimator.train_model\u001B[1;34m(self, training_data, validation_data, num_workers, prefetch_factor, shuffle_buffer_length, cache_data, **kwargs)\u001B[0m\n\u001B[0;32m    135\u001B[0m     validation_iter_dataset \u001B[38;5;241m=\u001B[39m TransformedIterableDataset(\n\u001B[0;32m    136\u001B[0m         dataset\u001B[38;5;241m=\u001B[39mvalidation_data,\n\u001B[0;32m    137\u001B[0m         transform\u001B[38;5;241m=\u001B[39mtransformation\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    141\u001B[0m         cache_data\u001B[38;5;241m=\u001B[39mcache_data,\n\u001B[0;32m    142\u001B[0m     )\n\u001B[0;32m    143\u001B[0m     validation_data_loader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[0;32m    144\u001B[0m         validation_iter_dataset,\n\u001B[0;32m    145\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    151\u001B[0m     )\n\u001B[1;32m--> 153\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrained_net\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransformation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransformation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m TrainOutput(\n\u001B[0;32m    164\u001B[0m     transformation\u001B[38;5;241m=\u001B[39mtransformation,\n\u001B[0;32m    165\u001B[0m     trained_net\u001B[38;5;241m=\u001B[39mtrained_net,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    168\u001B[0m     ),\n\u001B[0;32m    169\u001B[0m )\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\trainer.py:81\u001B[0m, in \u001B[0;36mTrainer.__call__\u001B[1;34m(self, net, train_iter, validation_iter, validation_dataset, transformation, estimator, device)\u001B[0m\n\u001B[0;32m     78\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     80\u001B[0m inputs \u001B[38;5;241m=\u001B[39m [v\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m data_entry\u001B[38;5;241m.\u001B[39mvalues()]\n\u001B[1;32m---> 81\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m     84\u001B[0m     loss \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\environments\\Miniconda3_py311_23.11.0-2\\envs\\py310t2cu118clone\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\mgtsd_network.py:437\u001B[0m, in \u001B[0;36mmgtsdTrainingNetwork.forward\u001B[1;34m(self, target_dimension_indicator, past_time_feat, past_target_cdf, past_observed_values, past_is_pad, future_time_feat, future_target_cdf, future_observed_values)\u001B[0m\n\u001B[0;32m    435\u001B[0m likelihoods \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    436\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ratio_index, share_ratio \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_ratio_list):\n\u001B[1;32m--> 437\u001B[0m     cur_likelihood \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiffusion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_prob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mratio_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m[\u001B[49m\u001B[43mratio_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m                                             \u001B[49m\u001B[43mshare_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshare_ratio\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    439\u001B[0m     likelihoods\u001B[38;5;241m.\u001B[39mappend(cur_likelihood)\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaling:\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\mgtsd_module.py:310\u001B[0m, in \u001B[0;36mGaussianDiffusion.log_prob\u001B[1;34m(self, x, cond, share_ratio, *args, **kwargs)\u001B[0m\n\u001B[0;32m    306\u001B[0m B, T, _ \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    308\u001B[0m time \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps,\n\u001B[0;32m    309\u001B[0m                      (B \u001B[38;5;241m*\u001B[39m T,), device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[1;32m--> 310\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_losses(\n\u001B[0;32m    311\u001B[0m     x\u001B[38;5;241m.\u001B[39mreshape(B \u001B[38;5;241m*\u001B[39m T, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), cond\u001B[38;5;241m.\u001B[39mreshape(B \u001B[38;5;241m*\u001B[39m T, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), time, share_ratio\u001B[38;5;241m=\u001B[39mshare_ratio,\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    313\u001B[0m )\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\mgtsd_module.py:292\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_losses\u001B[1;34m(self, x_start, cond, t, share_ratio, noise)\u001B[0m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;66;03m# x_t = a x0 + b \\eps\u001B[39;00m\n\u001B[0;32m    290\u001B[0m x_noisy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_sample(x_start\u001B[38;5;241m=\u001B[39mx_start, t\u001B[38;5;241m=\u001B[39mt,\n\u001B[0;32m    291\u001B[0m                         noise\u001B[38;5;241m=\u001B[39mnoise, share_ratio\u001B[38;5;241m=\u001B[39mshare_ratio)\n\u001B[1;32m--> 292\u001B[0m x_recon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdenoise_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_noisy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcond\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ml1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    295\u001B[0m     loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39ml1_loss(x_recon, noise)\n",
      "File \u001B[1;32mD:\\environments\\Miniconda3_py311_23.11.0-2\\envs\\py310t2cu118clone\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\epsilon_theta.py:146\u001B[0m, in \u001B[0;36mEpsilonTheta.forward\u001B[1;34m(self, inputs, time, cond)\u001B[0m\n\u001B[0;32m    144\u001B[0m skip \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresidual_layers:\n\u001B[1;32m--> 146\u001B[0m     x, skip_connection \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcond_up\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiffusion_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m     skip\u001B[38;5;241m.\u001B[39mappend(skip_connection)\n\u001B[0;32m    149\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(torch\u001B[38;5;241m.\u001B[39mstack(skip), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m/\u001B[39m \\\n\u001B[0;32m    150\u001B[0m     math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresidual_layers))  \u001B[38;5;66;03m# [B,8,T]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\environments\\Miniconda3_py311_23.11.0-2\\envs\\py310t2cu118clone\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\MyCode\\PyCharm_Code\\MG-TSD\\src\\epsilon_theta.py:61\u001B[0m, in \u001B[0;36mResidualBlock.forward\u001B[1;34m(self, x, conditioner, diffusion_step)\u001B[0m\n\u001B[0;32m     58\u001B[0m conditioner \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconditioner_projection(conditioner)\n\u001B[0;32m     60\u001B[0m y \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m diffusion_step\n\u001B[1;32m---> 61\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilated_conv\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconditioner\u001B[49m\n\u001B[0;32m     63\u001B[0m gate, \u001B[38;5;28mfilter\u001B[39m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mchunk(y, \u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     64\u001B[0m y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(gate) \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh(\u001B[38;5;28mfilter\u001B[39m)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 0; 16.00 GiB total capacity; 15.07 GiB already allocated; 0 bytes free; 15.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(\"================================================\")\n",
    "print(\"initlize the estimator\")\n",
    "\n",
    "estimator = mgtsdEstimator(\n",
    "    target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=dataset.metadata.prediction_length,\n",
    "    cell_type='GRU',\n",
    "    input_size=input_size,\n",
    "    freq=dataset.metadata.freq,\n",
    "    loss_type='l2',\n",
    "    scaling=True,\n",
    "    diff_steps=diff_steps,\n",
    "    share_ratio_list=share_ratio_list,\n",
    "    beta_end=0.1,\n",
    "    beta_schedule=\"linear\",\n",
    "    weights=weights,\n",
    "    num_cells=num_cells,\n",
    "    num_gran=num_gran,\n",
    "    trainer=Trainer(device=device,\n",
    "                    epochs=epoch,\n",
    "                    learning_rate=learning_rate,\n",
    "                    num_batches_per_epoch=100,\n",
    "                    batch_size=batch_size,\n",
    "                    log_matrics=False)\n",
    ")\n",
    "print(\"================================================\")\n",
    "print(\"start training the network\")\n",
    "predictor = estimator.train(\n",
    "   data_train, num_workers=0, validation_data=data_test,prefetch_factor=None)\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(\"make predictions\")\n",
    "forecast_it, ts_it = make_evaluation_predictions(dataset=data_test,\n",
    "                                                 predictor=predictor,\n",
    "                                                 num_samples=100)\n",
    "forecasts = list(forecast_it)\n",
    "targets = list(ts_it)\n",
    "\n",
    "targets_list = []\n",
    "forecasts_list = []\n",
    "target_dim = estimator.target_dim\n",
    "target_columns = targets[0].iloc[:, :target_dim].columns\n",
    "for cur_gran_index, cur_gran in enumerate(mg_dict):\n",
    "    targets_cur = []\n",
    "    predict_cur = []\n",
    "    predict_cur = copy.deepcopy(forecasts)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        targets_cur.append(\n",
    "            targets[i].iloc[:, (cur_gran_index * target_dim):((cur_gran_index + 1) * target_dim)])\n",
    "        targets_cur[-1].columns = target_columns\n",
    "    for day in range(len(forecasts)):\n",
    "        predict_cur[day].samples = forecasts[day].samples[:, :,\n",
    "                                                          (cur_gran_index * target_dim):((cur_gran_index + 1) * target_dim)]\n",
    "    targets_list.append(targets_cur)\n",
    "    forecasts_list.append(predict_cur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-03T08:29:16.789244500Z"
    }
   },
   "outputs": [],
   "source": [
    "plot(targets_list[0][0], forecasts_list[0][0], prediction_length=dataset.metadata.prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T08:29:16.794260300Z",
     "start_time": "2024-12-03T08:29:16.793760Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"===============================================\")\n",
    "print(\"evaluation metrics\")\n",
    "\n",
    "agg_metric_list = []\n",
    "for cur_gran_index, cur_gran in enumerate(mg_dict):\n",
    "    evaluator = MultivariateEvaluator(quantiles=(np.arange(20) / 20.0)[1:],\n",
    "                                      target_agg_funcs={'sum': np.sum})\n",
    "    agg_metric, item_metrics = evaluator(targets_list[cur_gran_index], forecasts_list[cur_gran_index],\n",
    "                                         num_series=len(data_test)/2)\n",
    "    agg_metric_list.append(agg_metric)\n",
    "    break # only evaluate the first gran\n",
    "\n",
    "for cur_gran_index, cur_gran in enumerate(mg_dict):\n",
    "    agg_metric = agg_metric_list[cur_gran_index]\n",
    "    print(f\"=======evaluation metrics for {cur_gran} h samples\")\n",
    "    print(\"CRPS:\", agg_metric[\"mean_wQuantileLoss\"])\n",
    "    print(\"ND:\", agg_metric[\"ND\"])\n",
    "    print(\"NRMSE:\", agg_metric[\"NRMSE\"])\n",
    "    print(\"\")\n",
    "    print(\"CRPS-Sum:\", agg_metric[\"m_sum_mean_wQuantileLoss\"])\n",
    "    print(\"ND-Sum:\", agg_metric[\"m_sum_ND\"])\n",
    "    print(\"NRMSE-Sum:\", agg_metric[\"m_sum_NRMSE\"])\n",
    "    break # only evaluate the first gran\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-03T08:29:16.794760100Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgtsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
